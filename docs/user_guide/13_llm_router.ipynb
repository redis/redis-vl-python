{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Routing\n",
    "\n",
    "RedisVL provides an `LLMRouter` that uses semantic similarity to route queries to the most appropriate LLM model tier. Instead of sending every request to your most expensive model, the router matches incoming queries against reference phrases to determine the right level of model capability.\n",
    "\n",
    "**The problem**: Applications default to using the most capable (and expensive) LLM for all queries, even when a simpler model would do just fine:\n",
    "- *\"Hello, how are you?\"* does **not** need Claude Opus 4.5 ($5/M input tokens)\n",
    "- *\"Hello, how are you?\"* is perfectly handled by GPT-4.1 Nano ($0.10/M input tokens)\n",
    "\n",
    "The `LLMRouter` solves this by classifying queries into **model tiers** (e.g., simple, standard, expert) using Redis vector search over a set of reference phrases that define each tier's \"semantic surface area.\"\n",
    "\n",
    "This notebook walks through every aspect of the LLM Router:\n",
    "1. Quick start with pretrained config\n",
    "2. Routing queries across tiers\n",
    "3. Defining custom tiers\n",
    "4. Cost-optimized routing\n",
    "5. Multi-match routing and aggregation methods\n",
    "6. Dynamic tier management\n",
    "7. Persistence and serialization\n",
    "8. Async usage\n",
    "9. LiteLLM integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:47.604936Z",
     "iopub.status.busy": "2026-02-16T19:52:47.604702Z",
     "iopub.status.idle": "2026-02-16T19:52:47.610313Z",
     "shell.execute_reply": "2026-02-16T19:52:47.609441Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\".*IProgress.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start with a Pretrained Config\n",
    "\n",
    "The fastest way to get started is with the built-in `\"default\"` pretrained configuration. It ships with **3 model tiers** and **pre-computed embeddings** (from `sentence-transformers/all-mpnet-base-v2`), so it loads instantly without needing to embed anything.\n",
    "\n",
    "The three tiers are grounded in [Bloom's Taxonomy](https://en.wikipedia.org/wiki/Bloom%27s_taxonomy) of cognitive complexity:\n",
    "\n",
    "| Tier | Bloom's Level | Model | Cost (input) | Example Tasks |\n",
    "|------|--------------|-------|-------------|---------------|\n",
    "| **simple** | Remember / Understand | `openai/gpt-4.1-nano` | $0.10/M | Greetings, factual QA, format conversion |\n",
    "| **standard** | Apply / Analyze | `anthropic/claude-sonnet-4-5` | $3.00/M | Code explanation, summarization, analysis |\n",
    "| **expert** | Evaluate / Create | `anthropic/claude-opus-4-5` | $5.00/M | Research, system architecture, formal proofs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:47.612428Z",
     "iopub.status.busy": "2026-02-16T19:52:47.612243Z",
     "iopub.status.idle": "2026-02-16T19:52:52.400728Z",
     "shell.execute_reply": "2026-02-16T19:52:52.400119Z"
    }
   },
   "outputs": [],
   "source": [
    "from redisvl.extensions.llm_router import LLMRouter\n",
    "\n",
    "router = LLMRouter.from_pretrained(\n",
    "    \"default\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:52.402530Z",
     "iopub.status.busy": "2026-02-16T19:52:52.402216Z",
     "iopub.status.idle": "2026-02-16T19:52:52.405414Z",
     "shell.execute_reply": "2026-02-16T19:52:52.404739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router name: llm-router-default\n",
      "Tier count: 3\n",
      "\n",
      "--- simple ---\n",
      "  Model:      openai/gpt-4.1-nano\n",
      "  References: 18 phrases\n",
      "  Threshold:  0.5\n",
      "  Cost (in):  $0.0001/1k tokens\n",
      "  Bloom's:    ['Remember', 'Understand']\n",
      "\n",
      "--- standard ---\n",
      "  Model:      anthropic/claude-sonnet-4-5\n",
      "  References: 18 phrases\n",
      "  Threshold:  0.6\n",
      "  Cost (in):  $0.003/1k tokens\n",
      "  Bloom's:    ['Apply', 'Analyze']\n",
      "\n",
      "--- expert ---\n",
      "  Model:      anthropic/claude-opus-4-5\n",
      "  References: 18 phrases\n",
      "  Threshold:  0.7\n",
      "  Cost (in):  $0.005/1k tokens\n",
      "  Bloom's:    ['Evaluate', 'Create']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect what was loaded\n",
    "print(\"Router name:\", router.name)\n",
    "print(\"Tier count:\", len(router.tiers))\n",
    "print()\n",
    "\n",
    "for tier in router.tiers:\n",
    "    print(f\"--- {tier.name} ---\")\n",
    "    print(f\"  Model:      {tier.model}\")\n",
    "    print(f\"  References: {len(tier.references)} phrases\")\n",
    "    print(f\"  Threshold:  {tier.distance_threshold}\")\n",
    "    print(f\"  Cost (in):  ${tier.metadata.get('cost_per_1k_input', 'N/A')}/1k tokens\")\n",
    "    print(f\"  Bloom's:    {tier.metadata.get('blooms_taxonomy', [])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:52.425188Z",
     "iopub.status.busy": "2026-02-16T19:52:52.425035Z",
     "iopub.status.idle": "2026-02-16T19:52:54.916612Z",
     "shell.execute_reply": "2026-02-16T19:52:54.916088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "Index Information:\r\n",
      "╭────────────────────────┬────────────────────────┬────────────────────────┬────────────────────────┬────────────────────────┬\b╮\r\n",
      "│ Index Name             │ Storage Type           │ Prefixes               │ Index Options          │ Indexing               │\r\n",
      "├────────────────────────┼────────────────────────┼────────────────────────┼────────────────────────┼────────────────────────┼\b┤\r\n",
      "| llm-router-default     | HASH                   | ['llm-router-default'] | []                     | 0                      |\r\n",
      "╰────────────────────────┴────────────────────────┴────────────────────────┴────────────────────────┴────────────────────────┴\b╯\r\n",
      "Index Fields:\r\n",
      "╭─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬\b╮\r\n",
      "│ Name            │ Attribute       │ Type            │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │\r\n",
      "├─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼\b┤\r\n",
      "│ reference_id    │ reference_id    │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │\r\n",
      "│ route_name      │ route_name      │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │\r\n",
      "│ reference       │ reference       │ TEXT            │ WEIGHT          │ 1               │                 │                 │                 │                 │                 │                 │\r\n",
      "│ vector          │ vector          │ VECTOR          │ algorithm       │ FLAT            │ data_type       │ FLOAT32         │ dim             │ 768             │ distance_metric │ COSINE          │\r\n",
      "╰─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴\b╯\r\n"
     ]
    }
   ],
   "source": [
    "# View the underlying Redis index\n",
    "!rvl index info -i llm-router-default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing Queries\n",
    "\n",
    "The `route()` method takes a text query, embeds it, and finds the best matching tier. It returns an `LLMRouteMatch` with the tier name, model string, distance, confidence, and any alternative matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:54.918679Z",
     "iopub.status.busy": "2026-02-16T19:52:54.918546Z",
     "iopub.status.idle": "2026-02-16T19:52:55.016173Z",
     "shell.execute_reply": "2026-02-16T19:52:55.015415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMRouteMatch(tier='simple', model='openai/gpt-4.1-nano', distance=0.382999330759, confidence=0.8085003346205, alternatives=[], metadata={'provider': 'openai', 'cost_per_1k_input': 0.0001, 'cost_per_1k_output': 0.0004, 'blooms_taxonomy': ['Remember', 'Understand']})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple greeting -> routes to the 'simple' tier\n",
    "match = router.route(\"hi, how are you doing today?\")\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:55.018045Z",
     "iopub.status.busy": "2026-02-16T19:52:55.017912Z",
     "iopub.status.idle": "2026-02-16T19:52:55.020345Z",
     "shell.execute_reply": "2026-02-16T19:52:55.019809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier:        simple\n",
      "Model:       openai/gpt-4.1-nano\n",
      "Distance:    0.3830\n",
      "Confidence:  0.8085\n",
      "Alternatives: []\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tier:        {match.tier}\")\n",
    "print(f\"Model:       {match.model}\")\n",
    "print(f\"Distance:    {match.distance:.4f}\")\n",
    "print(f\"Confidence:  {match.confidence:.4f}\")\n",
    "print(f\"Alternatives: {match.alternatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `distance` is the cosine distance (0-2, lower is closer). The `confidence` is derived as `1 - distance/2`, giving a 0-1 score where higher is better.\n",
    "\n",
    "The `alternatives` field shows other tiers that also matched, along with their distances. This is useful for understanding how close the decision was.\n",
    "\n",
    "Let's try more queries across the complexity spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:55.021857Z",
     "iopub.status.busy": "2026-02-16T19:52:55.021749Z",
     "iopub.status.idle": "2026-02-16T19:52:55.167448Z",
     "shell.execute_reply": "2026-02-16T19:52:55.167025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Simple Tier Queries ===\n",
      "  'what is the capital of France?'\n",
      "    -> simple (openai/gpt-4.1-nano) distance=0.1417\n",
      "  'thanks for your help!'\n",
      "    -> simple (openai/gpt-4.1-nano) distance=0.1270\n",
      "  'translate hello to Spanish'\n",
      "    -> simple (openai/gpt-4.1-nano) distance=0.2537\n"
     ]
    }
   ],
   "source": [
    "# Simple tier queries\n",
    "simple_queries = [\n",
    "    \"what is the capital of France?\",\n",
    "    \"thanks for your help!\",\n",
    "    \"translate hello to Spanish\",\n",
    "]\n",
    "\n",
    "print(\"=== Simple Tier Queries ===\")\n",
    "for q in simple_queries:\n",
    "    m = router.route(q)\n",
    "    print(f\"  '{q}'\")\n",
    "    print(f\"    -> {m.tier} ({m.model}) distance={m.distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:55.168616Z",
     "iopub.status.busy": "2026-02-16T19:52:55.168533Z",
     "iopub.status.idle": "2026-02-16T19:52:55.254869Z",
     "shell.execute_reply": "2026-02-16T19:52:55.254427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Standard Tier Queries ===\n",
      "  'explain how garbage collection works in Java'\n",
      "    -> standard (anthropic/claude-sonnet-4-5) distance=0.0000\n",
      "  'write unit tests for this Python class'\n",
      "    -> standard (anthropic/claude-sonnet-4-5) distance=0.4141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'compare and contrast microservices vs monolith architectures'\n",
      "    -> standard (anthropic/claude-sonnet-4-5) distance=0.1792\n"
     ]
    }
   ],
   "source": [
    "# Standard tier queries\n",
    "standard_queries = [\n",
    "    \"explain how garbage collection works in Java\",\n",
    "    \"write unit tests for this Python class\",\n",
    "    \"compare and contrast microservices vs monolith architectures\",\n",
    "]\n",
    "\n",
    "print(\"=== Standard Tier Queries ===\")\n",
    "for q in standard_queries:\n",
    "    m = router.route(q)\n",
    "    print(f\"  '{q}'\")\n",
    "    print(f\"    -> {m.tier} ({m.model}) distance={m.distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:55.256043Z",
     "iopub.status.busy": "2026-02-16T19:52:55.255949Z",
     "iopub.status.idle": "2026-02-16T19:52:55.361293Z",
     "shell.execute_reply": "2026-02-16T19:52:55.360892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Expert Tier Queries ===\n",
      "  'architect a fault-tolerant distributed database replication system'\n",
      "    -> expert (anthropic/claude-opus-4-5) distance=0.2583\n",
      "  'prove this mathematical theorem using formal methods'\n",
      "    -> expert (anthropic/claude-opus-4-5) distance=0.2947\n",
      "  'design a novel algorithm for NP-hard graph partitioning'\n",
      "    -> expert (anthropic/claude-opus-4-5) distance=0.2461\n"
     ]
    }
   ],
   "source": [
    "# Expert tier queries\n",
    "expert_queries = [\n",
    "    \"architect a fault-tolerant distributed database replication system\",\n",
    "    \"prove this mathematical theorem using formal methods\",\n",
    "    \"design a novel algorithm for NP-hard graph partitioning\",\n",
    "]\n",
    "\n",
    "print(\"=== Expert Tier Queries ===\")\n",
    "for q in expert_queries:\n",
    "    m = router.route(q)\n",
    "    print(f\"  '{q}'\")\n",
    "    print(f\"    -> {m.tier} ({m.model}) distance={m.distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:55.362372Z",
     "iopub.status.busy": "2026-02-16T19:52:55.362278Z",
     "iopub.status.idle": "2026-02-16T19:52:55.378550Z",
     "shell.execute_reply": "2026-02-16T19:52:55.378134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match: tier=None, model=None\n",
      "Bool check: False\n"
     ]
    }
   ],
   "source": [
    "# A query that doesn't match any tier returns an empty match\n",
    "match = router.route(\"xyzzy plugh random gibberish 12345 asdf\")\n",
    "print(f\"No match: tier={match.tier}, model={match.model}\")\n",
    "print(f\"Bool check: {bool(match)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:55.379840Z",
     "iopub.status.busy": "2026-02-16T19:52:55.379752Z",
     "iopub.status.idle": "2026-02-16T19:52:55.382825Z",
     "shell.execute_reply": "2026-02-16T19:52:55.382387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "router.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Tiers\n",
    "\n",
    "For production use, you'll want to define tiers tailored to your application. Each `ModelTier` specifies:\n",
    "\n",
    "- **`name`**: Unique identifier (e.g., `\"simple\"`, `\"coding\"`, `\"research\"`)\n",
    "- **`model`**: A [LiteLLM-compatible](https://docs.litellm.ai/docs/providers) model string (e.g., `\"anthropic/claude-sonnet-4-5\"`)\n",
    "- **`references`**: Example phrases that define this tier's semantic surface area. More references = better coverage.\n",
    "- **`metadata`**: Arbitrary dict for costs, capabilities, provider info, etc.\n",
    "- **`distance_threshold`**: Maximum cosine distance for matching (Redis COSINE: 0-2). Lower values require stricter matching.\n",
    "\n",
    "The quality of your reference phrases is the most important factor in routing accuracy. They should be representative of the *kinds* of queries you expect for each tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:55.384007Z",
     "iopub.status.busy": "2026-02-16T19:52:55.383929Z",
     "iopub.status.idle": "2026-02-16T19:52:57.463329Z",
     "shell.execute_reply": "2026-02-16T19:52:57.462824Z"
    }
   },
   "outputs": [],
   "source": [
    "from redisvl.extensions.llm_router import LLMRouter, ModelTier\n",
    "\n",
    "tiers = [\n",
    "    ModelTier(\n",
    "        name=\"simple\",\n",
    "        model=\"openai/gpt-4.1-nano\",\n",
    "        references=[\n",
    "            \"hello\",\n",
    "            \"hi there\",\n",
    "            \"thanks\",\n",
    "            \"goodbye\",\n",
    "            \"what time is it?\",\n",
    "            \"how are you?\",\n",
    "            \"yes\",\n",
    "            \"no\",\n",
    "            \"ok thanks\",\n",
    "        ],\n",
    "        metadata={\n",
    "            \"provider\": \"openai\",\n",
    "            \"cost_per_1k_input\": 0.0001,\n",
    "            \"cost_per_1k_output\": 0.0004,\n",
    "        },\n",
    "        distance_threshold=0.5,\n",
    "    ),\n",
    "    ModelTier(\n",
    "        name=\"reasoning\",\n",
    "        model=\"anthropic/claude-sonnet-4-5\",\n",
    "        references=[\n",
    "            \"analyze this code for bugs\",\n",
    "            \"explain how neural networks learn\",\n",
    "            \"compare and contrast these approaches\",\n",
    "            \"write a detailed blog post about\",\n",
    "            \"debug this issue step by step\",\n",
    "            \"summarize this research paper\",\n",
    "            \"write unit tests for this class\",\n",
    "            \"refactor this code for readability\",\n",
    "        ],\n",
    "        metadata={\n",
    "            \"provider\": \"anthropic\",\n",
    "            \"cost_per_1k_input\": 0.003,\n",
    "            \"cost_per_1k_output\": 0.015,\n",
    "        },\n",
    "        distance_threshold=0.6,\n",
    "    ),\n",
    "    ModelTier(\n",
    "        name=\"expert\",\n",
    "        model=\"anthropic/claude-opus-4-5\",\n",
    "        references=[\n",
    "            \"prove this mathematical theorem\",\n",
    "            \"architect a distributed system for millions of users\",\n",
    "            \"write a research paper analyzing\",\n",
    "            \"review this legal contract for issues\",\n",
    "            \"design a novel algorithm for\",\n",
    "            \"create a comprehensive security audit report\",\n",
    "        ],\n",
    "        metadata={\n",
    "            \"provider\": \"anthropic\",\n",
    "            \"cost_per_1k_input\": 0.005,\n",
    "            \"cost_per_1k_output\": 0.025,\n",
    "        },\n",
    "        distance_threshold=0.7,\n",
    "    ),\n",
    "]\n",
    "\n",
    "router = LLMRouter(\n",
    "    name=\"custom-router\",\n",
    "    tiers=tiers,\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:57.464842Z",
     "iopub.status.busy": "2026-02-16T19:52:57.464753Z",
     "iopub.status.idle": "2026-02-16T19:52:57.466880Z",
     "shell.execute_reply": "2026-02-16T19:52:57.466432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier names: ['simple', 'reasoning', 'expert']\n",
      "Thresholds: {'simple': 0.5, 'reasoning': 0.6, 'expert': 0.7}\n",
      "Default tier: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Tier names:\", router.tier_names)\n",
    "print(\"Thresholds:\", router.tier_thresholds)\n",
    "print(\"Default tier:\", router.default_tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:57.467928Z",
     "iopub.status.busy": "2026-02-16T19:52:57.467857Z",
     "iopub.status.idle": "2026-02-16T19:52:57.553009Z",
     "shell.execute_reply": "2026-02-16T19:52:57.552544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'simple' -> openai/gpt-4.1-nano\n",
      "'reasoning' -> anthropic/claude-sonnet-4-5\n",
      "'expert' -> anthropic/claude-opus-4-5\n"
     ]
    }
   ],
   "source": [
    "# Verify routing works\n",
    "match = router.route(\"hello, how are you?\")\n",
    "print(f\"'{match.tier}' -> {match.model}\")\n",
    "\n",
    "match = router.route(\"analyze this code for bugs and security issues\")\n",
    "print(f\"'{match.tier}' -> {match.model}\")\n",
    "\n",
    "match = router.route(\"design a fault-tolerant consensus protocol\")\n",
    "print(f\"'{match.tier}' -> {match.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-Computed Vectors\n",
    "\n",
    "If you've already embedded the query (e.g., from an upstream pipeline), you can pass the vector directly to avoid double-embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:57.554432Z",
     "iopub.status.busy": "2026-02-16T19:52:57.554334Z",
     "iopub.status.idle": "2026-02-16T19:52:57.606013Z",
     "shell.execute_reply": "2026-02-16T19:52:57.605576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computed vector route: simple (openai/gpt-4.1-nano)\n"
     ]
    }
   ],
   "source": [
    "vector = router.vectorizer.embed(\"hello\")\n",
    "match = router.route(vector=vector)\n",
    "print(f\"Pre-computed vector route: {match.tier} ({match.model})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-Optimized Routing\n",
    "\n",
    "When multiple tiers have similar semantic distances, cost optimization adds a penalty proportional to the model's cost. This biases the router toward cheaper tiers when the semantic match is close.\n",
    "\n",
    "The formula is:\n",
    "```\n",
    "adjusted_distance = distance + (cost_per_1k_input * cost_weight)\n",
    "```\n",
    "\n",
    "The `cost_weight` parameter (0-1) controls how much cost influences the decision. Default is 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:57.607212Z",
     "iopub.status.busy": "2026-02-16T19:52:57.607121Z",
     "iopub.status.idle": "2026-02-16T19:52:59.528838Z",
     "shell.execute_reply": "2026-02-16T19:52:59.528198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'help me understand this code'\n",
      "  Default routing:       reasoning (distance=0.5674)\n",
      "  Cost-optimized routing: reasoning (distance=0.5674)\n"
     ]
    }
   ],
   "source": [
    "from redisvl.extensions.llm_router.schema import RoutingConfig\n",
    "\n",
    "cost_router = LLMRouter(\n",
    "    name=\"cost-router\",\n",
    "    tiers=tiers,\n",
    "    routing_config=RoutingConfig(\n",
    "        cost_optimization=True,\n",
    "        cost_weight=0.3,  # Higher weight = stronger cost preference\n",
    "    ),\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Compare results with and without cost optimization\n",
    "query = \"help me understand this code\"\n",
    "\n",
    "match_default = router.route(query)\n",
    "match_cost = cost_router.route(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"  Default routing:       {match_default.tier} (distance={match_default.distance:.4f})\")\n",
    "print(f\"  Cost-optimized routing: {match_cost.tier} (distance={match_cost.distance:.4f})\")\n",
    "\n",
    "cost_router.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Match Routing (`route_many`)\n",
    "\n",
    "Use `route_many()` to get multiple tier matches, ordered by distance. This is useful when you want to see how a query scores across all tiers, or implement fallback logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.530151Z",
     "iopub.status.busy": "2026-02-16T19:52:59.530030Z",
     "iopub.status.idle": "2026-02-16T19:52:59.546988Z",
     "shell.execute_reply": "2026-02-16T19:52:59.546439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLMRouteMatch(tier='reasoning', model='anthropic/claude-sonnet-4-5', distance=0.524655163288, confidence=0.737672418356, alternatives=[], metadata={'provider': 'anthropic', 'cost_per_1k_input': 0.003, 'cost_per_1k_output': 0.015})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = router.route_many(\"explain machine learning concepts in detail\", max_k=3)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.548172Z",
     "iopub.status.busy": "2026-02-16T19:52:59.548092Z",
     "iopub.status.idle": "2026-02-16T19:52:59.550222Z",
     "shell.execute_reply": "2026-02-16T19:52:59.549712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  #1: reasoning (distance=0.5247, model=anthropic/claude-sonnet-4-5)\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(matches):\n",
    "    print(f\"  #{i+1}: {m.tier} (distance={m.distance:.4f}, model={m.model})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation Methods\n",
    "\n",
    "Each tier may have multiple reference phrases. The router aggregates distances across all matching references within a tier using one of three methods:\n",
    "\n",
    "- **`avg`** (default): Average distance across all matching references\n",
    "- **`min`**: Minimum distance (closest single reference match)\n",
    "- **`sum`**: Sum of all distances\n",
    "\n",
    "The `min` method is useful when you want a single strong match to be decisive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.551504Z",
     "iopub.status.busy": "2026-02-16T19:52:59.551416Z",
     "iopub.status.idle": "2026-02-16T19:52:59.577444Z",
     "shell.execute_reply": "2026-02-16T19:52:59.577100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG aggregation:\n",
      "  reasoning: distance=0.4042\n",
      "  expert: distance=0.6231\n",
      "\n",
      "MIN aggregation:\n",
      "  reasoning: distance=0.0758\n",
      "  expert: distance=0.5718\n"
     ]
    }
   ],
   "source": [
    "from redisvl.extensions.llm_router.schema import DistanceAggregationMethod\n",
    "\n",
    "query = \"analyze this code and find potential bugs\"\n",
    "\n",
    "# Default: avg aggregation\n",
    "matches_avg = router.route_many(query, max_k=3, aggregation_method=DistanceAggregationMethod.avg)\n",
    "# Min aggregation\n",
    "matches_min = router.route_many(query, max_k=3, aggregation_method=DistanceAggregationMethod.min)\n",
    "\n",
    "print(\"AVG aggregation:\")\n",
    "for m in matches_avg:\n",
    "    print(f\"  {m.tier}: distance={m.distance:.4f}\")\n",
    "\n",
    "print(\"\\nMIN aggregation:\")\n",
    "for m in matches_min:\n",
    "    print(f\"  {m.tier}: distance={m.distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the different distances: with `min`, the distance reflects the single closest reference, while `avg` averages across all matching references in each tier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Tier Management\n",
    "\n",
    "Tiers can be added, removed, and updated at runtime without recreating the router."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.578968Z",
     "iopub.status.busy": "2026-02-16T19:52:59.578880Z",
     "iopub.status.idle": "2026-02-16T19:52:59.644796Z",
     "shell.execute_reply": "2026-02-16T19:52:59.644245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiers after add: ['simple', 'reasoning', 'expert', 'local']\n"
     ]
    }
   ],
   "source": [
    "local_tier = ModelTier(\n",
    "    name=\"local\",\n",
    "    model=\"ollama/llama3.2\",\n",
    "    references=[\"ok\", \"sure\", \"yes\", \"no\", \"got it\"],\n",
    "    metadata={\"provider\": \"ollama\", \"cost_per_1k_input\": 0},\n",
    "    distance_threshold=0.3,\n",
    ")\n",
    "router.add_tier(local_tier)\n",
    "print(\"Tiers after add:\", router.tier_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add references to an existing tier\n",
    "\n",
    "If a tier's semantic coverage is too narrow, you can expand it by adding more reference phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.645946Z",
     "iopub.status.busy": "2026-02-16T19:52:59.645856Z",
     "iopub.status.idle": "2026-02-16T19:52:59.713364Z",
     "shell.execute_reply": "2026-02-16T19:52:59.712925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple tier now has 12 references:\n",
      "  - hello\n",
      "  - hi there\n",
      "  - thanks\n",
      "  - goodbye\n",
      "  - what time is it?\n",
      "  - how are you?\n",
      "  - yes\n",
      "  - no\n",
      "  - ok thanks\n",
      "  - howdy partner\n",
      "  - greetings friend\n",
      "  - hey what's up\n"
     ]
    }
   ],
   "source": [
    "router.add_tier_references(\n",
    "    tier_name=\"simple\",\n",
    "    references=[\"howdy partner\", \"greetings friend\", \"hey what's up\"]\n",
    ")\n",
    "\n",
    "tier = router.get_tier(\"simple\")\n",
    "print(f\"Simple tier now has {len(tier.references)} references:\")\n",
    "for ref in tier.references:\n",
    "    print(f\"  - {ref}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update a tier's distance threshold\n",
    "\n",
    "You can tune the strictness of matching per tier. Lower thresholds require closer matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.714755Z",
     "iopub.status.busy": "2026-02-16T19:52:59.714669Z",
     "iopub.status.idle": "2026-02-16T19:52:59.717968Z",
     "shell.execute_reply": "2026-02-16T19:52:59.717564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: {'simple': 0.5, 'reasoning': 0.6, 'expert': 0.7, 'local': 0.3}\n",
      "After: {'simple': 0.4, 'reasoning': 0.6, 'expert': 0.7, 'local': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", router.tier_thresholds)\n",
    "\n",
    "router.update_tier_threshold(\"simple\", 0.4)  # Stricter matching\n",
    "\n",
    "print(\"After:\", router.tier_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove a tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.719169Z",
     "iopub.status.busy": "2026-02-16T19:52:59.719096Z",
     "iopub.status.idle": "2026-02-16T19:52:59.726151Z",
     "shell.execute_reply": "2026-02-16T19:52:59.725841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiers after remove: ['simple', 'reasoning', 'expert']\n",
      "Thresholds: {'simple': 0.5, 'reasoning': 0.6, 'expert': 0.7}\n"
     ]
    }
   ],
   "source": [
    "router.remove_tier(\"local\")\n",
    "print(\"Tiers after remove:\", router.tier_names)\n",
    "\n",
    "# Reset the simple threshold back to 0.5 for the rest of the demo\n",
    "router.update_tier_threshold(\"simple\", 0.5)\n",
    "print(\"Thresholds:\", router.tier_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve a tier by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.727367Z",
     "iopub.status.busy": "2026-02-16T19:52:59.727279Z",
     "iopub.status.idle": "2026-02-16T19:52:59.729452Z",
     "shell.execute_reply": "2026-02-16T19:52:59.728966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier: reasoning\n",
      "Model: anthropic/claude-sonnet-4-5\n",
      "Threshold: 0.6\n",
      "References: ['analyze this code for bugs', 'explain how neural networks learn', 'compare and contrast these approaches', 'write a detailed blog post about', 'debug this issue step by step', 'summarize this research paper', 'write unit tests for this class', 'refactor this code for readability']\n",
      "\n",
      "Non-existent: None\n"
     ]
    }
   ],
   "source": [
    "tier = router.get_tier(\"reasoning\")\n",
    "print(f\"Tier: {tier.name}\")\n",
    "print(f\"Model: {tier.model}\")\n",
    "print(f\"Threshold: {tier.distance_threshold}\")\n",
    "print(f\"References: {tier.references}\")\n",
    "\n",
    "# Non-existent tier returns None\n",
    "print(f\"\\nNon-existent: {router.get_tier('nonexistent')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence and Serialization\n",
    "\n",
    "Routers can be serialized and restored in several formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary round-trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.731013Z",
     "iopub.status.busy": "2026-02-16T19:52:59.730912Z",
     "iopub.status.idle": "2026-02-16T19:52:59.733431Z",
     "shell.execute_reply": "2026-02-16T19:52:59.733079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'custom-router',\n",
       " 'tiers': [{'name': 'simple',\n",
       "   'model': 'openai/gpt-4.1-nano',\n",
       "   'references': ['hello',\n",
       "    'hi there',\n",
       "    'thanks',\n",
       "    'goodbye',\n",
       "    'what time is it?',\n",
       "    'how are you?',\n",
       "    'yes',\n",
       "    'no',\n",
       "    'ok thanks',\n",
       "    'howdy partner',\n",
       "    'greetings friend',\n",
       "    \"hey what's up\"],\n",
       "   'metadata': {'provider': 'openai',\n",
       "    'cost_per_1k_input': 0.0001,\n",
       "    'cost_per_1k_output': 0.0004},\n",
       "   'distance_threshold': 0.5},\n",
       "  {'name': 'reasoning',\n",
       "   'model': 'anthropic/claude-sonnet-4-5',\n",
       "   'references': ['analyze this code for bugs',\n",
       "    'explain how neural networks learn',\n",
       "    'compare and contrast these approaches',\n",
       "    'write a detailed blog post about',\n",
       "    'debug this issue step by step',\n",
       "    'summarize this research paper',\n",
       "    'write unit tests for this class',\n",
       "    'refactor this code for readability'],\n",
       "   'metadata': {'provider': 'anthropic',\n",
       "    'cost_per_1k_input': 0.003,\n",
       "    'cost_per_1k_output': 0.015},\n",
       "   'distance_threshold': 0.6},\n",
       "  {'name': 'expert',\n",
       "   'model': 'anthropic/claude-opus-4-5',\n",
       "   'references': ['prove this mathematical theorem',\n",
       "    'architect a distributed system for millions of users',\n",
       "    'write a research paper analyzing',\n",
       "    'review this legal contract for issues',\n",
       "    'design a novel algorithm for',\n",
       "    'create a comprehensive security audit report'],\n",
       "   'metadata': {'provider': 'anthropic',\n",
       "    'cost_per_1k_input': 0.005,\n",
       "    'cost_per_1k_output': 0.025},\n",
       "   'distance_threshold': 0.7}],\n",
       " 'vectorizer': {'type': 'hf',\n",
       "  'model': 'sentence-transformers/all-mpnet-base-v2'},\n",
       " 'routing_config': {'max_k': 1,\n",
       "  'aggregation_method': 'avg',\n",
       "  'cost_optimization': False,\n",
       "  'cost_weight': 0.1}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_dict = router.to_dict()\n",
    "router_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:52:59.734601Z",
     "iopub.status.busy": "2026-02-16T19:52:59.734513Z",
     "iopub.status.idle": "2026-02-16T19:53:01.695457Z",
     "shell.execute_reply": "2026-02-16T19:53:01.694956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict round-trip OK\n"
     ]
    }
   ],
   "source": [
    "# Restore from dict (reconnects to same Redis index since name matches)\n",
    "router_from_dict = LLMRouter.from_dict(\n",
    "    router_dict,\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    ")\n",
    "\n",
    "assert router_from_dict.to_dict() == router.to_dict()\n",
    "print(\"Dict round-trip OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:01.696656Z",
     "iopub.status.busy": "2026-02-16T19:53:01.696568Z",
     "iopub.status.idle": "2026-02-16T19:53:03.900573Z",
     "shell.execute_reply": "2026-02-16T19:53:03.900028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML round-trip OK\n"
     ]
    }
   ],
   "source": [
    "router.to_yaml(\"llm_router.yaml\", overwrite=True)\n",
    "\n",
    "router_from_yaml = LLMRouter.from_yaml(\n",
    "    \"llm_router.yaml\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    ")\n",
    "\n",
    "assert router_from_yaml.to_dict() == router.to_dict()\n",
    "print(\"YAML round-trip OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconnect to an existing router\n",
    "\n",
    "If the router's Redis index still exists, you can reconnect without needing the original config. The router config is persisted in Redis alongside the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:03.902226Z",
     "iopub.status.busy": "2026-02-16T19:53:03.902108Z",
     "iopub.status.idle": "2026-02-16T19:53:05.844833Z",
     "shell.execute_reply": "2026-02-16T19:53:05.844339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconnected to 'custom-router' with tiers: ['simple', 'reasoning', 'expert']\n",
      "Route test: simple (openai/gpt-4.1-nano)\n"
     ]
    }
   ],
   "source": [
    "router_reconnected = LLMRouter.from_existing(\n",
    "    name=\"custom-router\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    ")\n",
    "\n",
    "print(f\"Reconnected to '{router_reconnected.name}' with tiers: {router_reconnected.tier_names}\")\n",
    "\n",
    "# Routing still works\n",
    "match = router_reconnected.route(\"hi there, how are you?\")\n",
    "print(f\"Route test: {match.tier} ({match.model})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export/Import with pre-computed embeddings\n",
    "\n",
    "For sharing router configs across environments (or loading without an embedding model), you can export with pre-computed vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:05.846163Z",
     "iopub.status.busy": "2026-02-16T19:53:05.846064Z",
     "iopub.status.idle": "2026-02-16T19:53:06.066988Z",
     "shell.execute_reply": "2026-02-16T19:53:06.066533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported with embeddings\n"
     ]
    }
   ],
   "source": [
    "# Export: embeds all references and saves vectors alongside text\n",
    "router.export_with_embeddings(\"my_router_pretrained.json\")\n",
    "print(\"Exported with embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:06.068165Z",
     "iopub.status.busy": "2026-02-16T19:53:06.068028Z",
     "iopub.status.idle": "2026-02-16T19:53:06.075194Z",
     "shell.execute_reply": "2026-02-16T19:53:06.074736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config name: custom-router\n",
      "Vectorizer: {'type': 'hf', 'model': 'sentence-transformers/all-mpnet-base-v2'}\n",
      "Tiers: 3\n",
      "First reference vector length: 768\n"
     ]
    }
   ],
   "source": [
    "# Peek at the structure\n",
    "import json\n",
    "\n",
    "with open(\"my_router_pretrained.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Config name: {data['name']}\")\n",
    "print(f\"Vectorizer: {data['vectorizer']}\")\n",
    "print(f\"Tiers: {len(data['tiers'])}\")\n",
    "print(f\"First reference vector length: {len(data['tiers'][0]['references'][0]['vector'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:06.076443Z",
     "iopub.status.busy": "2026-02-16T19:53:06.076358Z",
     "iopub.status.idle": "2026-02-16T19:53:08.058648Z",
     "shell.execute_reply": "2026-02-16T19:53:08.058112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported router route: simple (openai/gpt-4.1-nano)\n"
     ]
    }
   ],
   "source": [
    "# Import: loads pre-computed vectors directly, no embedding needed\n",
    "router_imported = LLMRouter.from_pretrained(\n",
    "    \"my_router_pretrained.json\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    ")\n",
    "\n",
    "match = router_imported.route(\"hi there, how are you?\")\n",
    "print(f\"Imported router route: {match.tier} ({match.model})\")\n",
    "\n",
    "# Note: router_imported shares the same Redis index as `router`\n",
    "# (same name in the exported config), so we don't delete it separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Usage\n",
    "\n",
    "The `AsyncLLMRouter` provides the same functionality using async I/O. Since Python's `__init__` can't be async, use the `create()` classmethod factory to instantiate, or `from_pretrained()` / `from_existing()` which are also async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:08.060383Z",
     "iopub.status.busy": "2026-02-16T19:53:08.060271Z",
     "iopub.status.idle": "2026-02-16T19:53:10.205348Z",
     "shell.execute_reply": "2026-02-16T19:53:10.204936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async router tiers: ['simple', 'standard', 'expert']\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"redisvl.utils.vectorize.base\").setLevel(logging.ERROR)\n",
    "\n",
    "from redisvl.extensions.llm_router import AsyncLLMRouter, ModelTier\n",
    "\n",
    "# Create from pretrained (async)\n",
    "async_router = await AsyncLLMRouter.from_pretrained(\n",
    "    \"default\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    ")\n",
    "\n",
    "print(f\"Async router tiers: {async_router.tier_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:10.206777Z",
     "iopub.status.busy": "2026-02-16T19:53:10.206687Z",
     "iopub.status.idle": "2026-02-16T19:53:10.268253Z",
     "shell.execute_reply": "2026-02-16T19:53:10.267841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple: simple (openai/gpt-4.1-nano)\n",
      "Standard: standard (anthropic/claude-sonnet-4-5)\n",
      "Expert: expert (anthropic/claude-opus-4-5)\n"
     ]
    }
   ],
   "source": [
    "# Route queries (async)\n",
    "match = await async_router.route(\"hi, how are you?\")\n",
    "print(f\"Simple: {match.tier} ({match.model})\")\n",
    "\n",
    "match = await async_router.route(\"explain how neural networks learn\")\n",
    "print(f\"Standard: {match.tier} ({match.model})\")\n",
    "\n",
    "match = await async_router.route(\"architect a fault-tolerant distributed system\")\n",
    "print(f\"Expert: {match.tier} ({match.model})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:10.269710Z",
     "iopub.status.busy": "2026-02-16T19:53:10.269626Z",
     "iopub.status.idle": "2026-02-16T19:53:10.285633Z",
     "shell.execute_reply": "2026-02-16T19:53:10.285212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  standard: distance=0.5297\n",
      "  expert: distance=0.6355\n"
     ]
    }
   ],
   "source": [
    "# Route many (async)\n",
    "matches = await async_router.route_many(\"summarize this research paper\", max_k=3)\n",
    "for m in matches:\n",
    "    print(f\"  {m.tier}: distance={m.distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:10.286702Z",
     "iopub.status.busy": "2026-02-16T19:53:10.286630Z",
     "iopub.status.idle": "2026-02-16T19:53:12.303496Z",
     "shell.execute_reply": "2026-02-16T19:53:12.302961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom async: fast (openai/gpt-4o-mini)\n"
     ]
    }
   ],
   "source": [
    "# Or create with custom tiers (async)\n",
    "custom_async = await AsyncLLMRouter.create(\n",
    "    name=\"async-custom\",\n",
    "    tiers=[\n",
    "        ModelTier(\n",
    "            name=\"fast\",\n",
    "            model=\"openai/gpt-4o-mini\",\n",
    "            references=[\"hello\", \"thanks\", \"what is\"],\n",
    "            distance_threshold=0.5,\n",
    "        ),\n",
    "        ModelTier(\n",
    "            name=\"smart\",\n",
    "            model=\"openai/gpt-4o\",\n",
    "            references=[\"analyze this\", \"explain how\", \"compare these\"],\n",
    "            distance_threshold=0.6,\n",
    "        ),\n",
    "    ],\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "match = await custom_async.route(\"hi there!\")\n",
    "print(f\"Custom async: {match.tier} ({match.model})\")\n",
    "\n",
    "await custom_async.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:12.304756Z",
     "iopub.status.busy": "2026-02-16T19:53:12.304643Z",
     "iopub.status.idle": "2026-02-16T19:53:12.307669Z",
     "shell.execute_reply": "2026-02-16T19:53:12.307346Z"
    }
   },
   "outputs": [],
   "source": [
    "await async_router.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteLLM Integration\n",
    "\n",
    "The router returns LiteLLM-compatible model strings, making integration straightforward. Here's a typical pattern:\n",
    "\n",
    "```python\n",
    "from litellm import completion\n",
    "from redisvl.extensions.llm_router import LLMRouter\n",
    "\n",
    "router = LLMRouter.from_pretrained(\"default\", redis_url=\"redis://localhost:6379\")\n",
    "\n",
    "def smart_completion(query: str, **kwargs):\n",
    "    \"\"\"Route to the best model, then call it.\"\"\"\n",
    "    match = router.route(query)\n",
    "    \n",
    "    if not match:\n",
    "        # Fallback to a default model if no tier matched\n",
    "        model = \"anthropic/claude-sonnet-4-5\"\n",
    "    else:\n",
    "        model = match.model\n",
    "        print(f\"Routed to {match.tier} tier ({model}), confidence={match.confidence:.2f}\")\n",
    "    \n",
    "    return completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "# Simple query -> GPT-4.1 Nano ($0.10/M)\n",
    "response = smart_completion(\"What is 2 + 2?\")\n",
    "\n",
    "# Complex query -> Opus 4.5 ($5/M)\n",
    "response = smart_completion(\"Design a distributed consensus algorithm with Byzantine fault tolerance\")\n",
    "```\n",
    "\n",
    "For async applications:\n",
    "\n",
    "```python\n",
    "from litellm import acompletion\n",
    "from redisvl.extensions.llm_router import AsyncLLMRouter\n",
    "\n",
    "router = await AsyncLLMRouter.from_pretrained(\"default\", redis_url=\"redis://localhost:6379\")\n",
    "\n",
    "async def smart_completion(query: str, **kwargs):\n",
    "    match = await router.route(query)\n",
    "    model = match.model if match else \"anthropic/claude-sonnet-4-5\"\n",
    "    return await acompletion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        **kwargs,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:12.308982Z",
     "iopub.status.busy": "2026-02-16T19:53:12.308906Z",
     "iopub.status.idle": "2026-02-16T19:53:12.312105Z",
     "shell.execute_reply": "2026-02-16T19:53:12.311785Z"
    }
   },
   "outputs": [],
   "source": [
    "router.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:53:12.313298Z",
     "iopub.status.busy": "2026-02-16T19:53:12.313229Z",
     "iopub.status.idle": "2026-02-16T19:53:12.315299Z",
     "shell.execute_reply": "2026-02-16T19:53:12.314991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove temp files\n",
    "import os\n",
    "for f in [\"llm_router.yaml\", \"my_router_pretrained.json\"]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
