{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching Embeddings\n",
    "\n",
    "RedisVL provides an `EmbeddingsCache` that makes it easy to store and retrieve embedding vectors with their associated text and metadata. This cache is particularly useful for applications that frequently compute the same embeddings, enabling you to:\n",
    "\n",
    "- Reduce computational costs by reusing previously computed embeddings\n",
    "- Decrease latency in applications that rely on embeddings\n",
    "- Store additional metadata alongside embeddings for richer applications\n",
    "\n",
    "This notebook will show you how to use the `EmbeddingsCache` effectively in your applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries. We'll use a text embedding model from HuggingFace to generate our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Disable tokenizers parallelism to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "\n",
    "# Import the EmbeddingsCache\n",
    "from redisvl.extensions.cache.embeddings import EmbeddingsCache\n",
    "from redisvl.utils.vectorize import HFTextVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a vectorizer to generate embeddings for our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = HFTextVectorizer(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    cache_folder=os.getenv(\"SENTENCE_TRANSFORMERS_HOME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the EmbeddingsCache\n",
    "\n",
    "Now let's initialize our `EmbeddingsCache`. The cache requires a Redis connection to store the embeddings and their associated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embeddings cache\n",
    "cache = EmbeddingsCache(\n",
    "    name=\"embedcache\",                  # name prefix for Redis keys\n",
    "    redis_url=\"redis://localhost:6379\",  # Redis connection URL\n",
    "    ttl=None                            # Optional TTL in seconds (None means no expiration)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "### Storing Embeddings\n",
    "\n",
    "Let's store some text with its embedding in the cache. The `set` method takes the following parameters:\n",
    "- `text`: The input text that was embedded\n",
    "- `model_name`: The name of the embedding model used\n",
    "- `embedding`: The embedding vector\n",
    "- `metadata`: Optional metadata associated with the embedding\n",
    "- `ttl`: Optional time-to-live override for this specific entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored with key: embedcache:a1b2c3d4...\n"
     ]
    }
   ],
   "source": [
    "# Text to embed\n",
    "text = \"What is machine learning?\"\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Generate the embedding\n",
    "embedding = vectorizer.embed(text)\n",
    "\n",
    "# Optional metadata\n",
    "metadata = {\"category\": \"ai\", \"source\": \"user_query\"}\n",
    "\n",
    "# Store in cache\n",
    "key = cache.set(\n",
    "    text=text,\n",
    "    model_name=model_name,\n",
    "    embedding=embedding,\n",
    "    metadata=metadata\n",
    ")\n",
    "\n",
    "print(f\"Stored with key: {key[:15]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Embeddings\n",
    "\n",
    "To retrieve an embedding from the cache, use the `get` method with the original text and model name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in cache: What is machine learning?\n",
      "Model: sentence-transformers/all-mpnet-base-v2\n",
      "Metadata: {'category': 'ai', 'source': 'user_query'}\n",
      "Embedding shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve from cache\n",
    "\n",
    "if result := cache.get(text=text, model_name=model_name):\n",
    "    print(f\"Found in cache: {result['text']}\")\n",
    "    print(f\"Model: {result['model_name']}\")\n",
    "    print(f\"Metadata: {result['metadata']}\")\n",
    "    print(f\"Embedding shape: {np.array(result['embedding']).shape}\")\n",
    "else:\n",
    "    print(\"Not found in cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Existence\n",
    "\n",
    "You can check if an embedding exists in the cache without retrieving it using the `exists` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First query exists in cache: True\n",
      "New query exists in cache: False\n"
     ]
    }
   ],
   "source": [
    "# Check if existing text is in cache\n",
    "exists = cache.exists(text=text, model_name=model_name)\n",
    "print(f\"First query exists in cache: {exists}\")\n",
    "\n",
    "# Check if a new text is in cache\n",
    "new_text = \"What is deep learning?\"\n",
    "exists = cache.exists(text=new_text, model_name=model_name)\n",
    "print(f\"New query exists in cache: {exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Entries\n",
    "\n",
    "To remove an entry from the cache, use the `drop` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping: False\n"
     ]
    }
   ],
   "source": [
    "# Remove from cache\n",
    "cache.drop(text=text, model_name=model_name)\n",
    "\n",
    "# Verify it's gone\n",
    "exists = cache.exists(text=text, model_name=model_name)\n",
    "print(f\"After dropping: {exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "### Key-Based Operations\n",
    "\n",
    "The `EmbeddingsCache` also provides methods that work directly with Redis keys, which can be useful for advanced use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored with key: embedcache:a1b2c3d4...\n",
      "Exists by key: True\n",
      "Retrieved by key: What is machine learning?\n"
     ]
    }
   ],
   "source": [
    "# Store an entry again\n",
    "key = cache.set(\n",
    "    text=text,\n",
    "    model_name=model_name,\n",
    "    embedding=embedding,\n",
    "    metadata=metadata\n",
    ")\n",
    "print(f\"Stored with key: {key[:15]}...\")\n",
    "\n",
    "# Check existence by key\n",
    "exists_by_key = cache.exists_by_key(key)\n",
    "print(f\"Exists by key: {exists_by_key}\")\n",
    "\n",
    "# Retrieve by key\n",
    "result_by_key = cache.get_by_key(key)\n",
    "print(f\"Retrieved by key: {result_by_key['text']}\")\n",
    "\n",
    "# Drop by key\n",
    "cache.drop_by_key(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with TTL (Time-To-Live)\n",
    "\n",
    "You can set a global TTL when initializing the cache, or specify TTL for individual entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immediately after setting: True\n",
      "After waiting: False\n"
     ]
    }
   ],
   "source": [
    "# Create a cache with a default 5-second TTL\n",
    "ttl_cache = EmbeddingsCache(\n",
    "    name=\"ttl_cache\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    ttl=5  # 5 second TTL\n",
    ")\n",
    "\n",
    "# Store an entry\n",
    "key = ttl_cache.set(\n",
    "    text=text,\n",
    "    model_name=model_name,\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "# Check if it exists\n",
    "exists = ttl_cache.exists_by_key(key)\n",
    "print(f\"Immediately after setting: {exists}\")\n",
    "\n",
    "# Wait for it to expire\n",
    "time.sleep(6)\n",
    "\n",
    "# Check again\n",
    "exists = ttl_cache.exists_by_key(key)\n",
    "print(f\"After waiting: {exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also override the default TTL for individual entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry with custom TTL after 2 seconds: False\n",
      "Entry with default TTL after 2 seconds: True\n"
     ]
    }
   ],
   "source": [
    "# Store an entry with a custom 1-second TTL\n",
    "key1 = ttl_cache.set(\n",
    "    text=\"Short-lived entry\",\n",
    "    model_name=model_name,\n",
    "    embedding=embedding,\n",
    "    ttl=1  # Override with 1 second TTL\n",
    ")\n",
    "\n",
    "# Store another entry with the default TTL (5 seconds)\n",
    "key2 = ttl_cache.set(\n",
    "    text=\"Default TTL entry\",\n",
    "    model_name=model_name,\n",
    "    embedding=embedding\n",
    "    # No TTL specified = uses the default 5 seconds\n",
    ")\n",
    "\n",
    "# Wait for 2 seconds\n",
    "time.sleep(2)\n",
    "\n",
    "# Check both entries\n",
    "exists1 = ttl_cache.exists_by_key(key1)\n",
    "exists2 = ttl_cache.exists_by_key(key2)\n",
    "\n",
    "print(f\"Entry with custom TTL after 2 seconds: {exists1}\")\n",
    "print(f\"Entry with default TTL after 2 seconds: {exists2}\")\n",
    "\n",
    "# Cleanup\n",
    "ttl_cache.drop_by_key(key2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Support\n",
    "\n",
    "The `EmbeddingsCache` provides async versions of all methods for use in async applications. The async methods are prefixed with `a` (e.g., `aset`, `aget`, `aexists`, `adrop`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async set successful? True\n",
      "Async get successful? True\n"
     ]
    }
   ],
   "source": [
    "async def async_cache_demo():\n",
    "    # Store an entry asynchronously\n",
    "    key = await cache.aset(\n",
    "        text=\"Async embedding\",\n",
    "        model_name=model_name,\n",
    "        embedding=embedding,\n",
    "        metadata={\"async\": True}\n",
    "    )\n",
    "    \n",
    "    # Check if it exists\n",
    "    exists = await cache.aexists_by_key(key)\n",
    "    print(f\"Async set successful? {exists}\")\n",
    "    \n",
    "    # Retrieve it\n",
    "    result = await cache.aget_by_key(key)\n",
    "    success = result is not None and result[\"text\"] == \"Async embedding\"\n",
    "    print(f\"Async get successful? {success}\")\n",
    "    \n",
    "    # Remove it\n",
    "    await cache.adrop_by_key(key)\n",
    "\n",
    "# Run the async demo\n",
    "await async_cache_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Example\n",
    "\n",
    "Let's build a simple embeddings caching system for a text classification task. We'll check the cache before computing new embeddings to save computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embedding for: How does machine learning work?\n",
      "Found in cache: What is artificial intelligence?\n",
      "Computing embedding for: What are neural networks?\n",
      "Found in cache: How does machine learning work?\n",
      "Found in cache: What are neural networks?\n",
      "\n",
      "Statistics:\n",
      "Total queries: 5\n",
      "Cache hits: 3\n",
      "Cache misses: 2\n",
      "Cache hit rate: 60.0%\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh cache for this example\n",
    "example_cache = EmbeddingsCache(\n",
    "    name=\"example_cache\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    ttl=3600  # 1 hour TTL\n",
    ")\n",
    "\n",
    "# Function to get embedding with caching\n",
    "def get_cached_embedding(text, model_name):\n",
    "    # Check if it's in the cache first\n",
    "    if cached_result := example_cache.get(text=text, model_name=model_name):\n",
    "        print(f\"Found in cache: {text}\")\n",
    "        return cached_result[\"embedding\"]\n",
    "    \n",
    "    # Not in cache, compute the embedding\n",
    "    print(f\"Computing embedding for: {text}\")\n",
    "    embedding = vectorizer.embed(text)\n",
    "    \n",
    "    # Store in cache\n",
    "    example_cache.set(\n",
    "        text=text,\n",
    "        model_name=model_name,\n",
    "        embedding=embedding,\n",
    "    )\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Simulate processing a stream of queries\n",
    "queries = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"What is artificial intelligence?\",  # Repeated query\n",
    "    \"What are neural networks?\",\n",
    "    \"How does machine learning work?\"   # Repeated query\n",
    "]\n",
    "\n",
    "# Process the queries and track statistics\n",
    "total_queries = 0\n",
    "cache_hits = 0\n",
    "\n",
    "for query in queries:\n",
    "    total_queries += 1\n",
    "    \n",
    "    # Check cache before computing\n",
    "    before = example_cache.exists(text=query, model_name=model_name)\n",
    "    if before:\n",
    "        cache_hits += 1\n",
    "    \n",
    "    # Get embedding (will compute or use cache)\n",
    "    embedding = get_cached_embedding(query, model_name)\n",
    "\n",
    "# Report statistics\n",
    "cache_misses = total_queries - cache_hits\n",
    "hit_rate = (cache_hits / total_queries) * 100\n",
    "\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total queries: {total_queries}\")\n",
    "print(f\"Cache hits: {cache_hits}\")\n",
    "print(f\"Cache misses: {cache_misses}\")\n",
    "print(f\"Cache hit rate: {hit_rate:.1f}%\")\n",
    "\n",
    "# Cleanup\n",
    "for query in set(queries):  # Use set to get unique queries\n",
    "    example_cache.drop(text=query, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark\n",
    "\n",
    "Let's run a benchmark to compare the performance of embedding with and without caching. We'll measure the time it takes to process the same query multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking without caching:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8a7d74c5de4f398dce784ca50b24e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken without caching: 0.8720 seconds\n",
      "Average time per embedding: 0.0872 seconds\n",
      "\n",
      "Benchmarking with caching:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8a7d74c5de4f398dce784ca50b24e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with caching: 0.0524 seconds\n",
      "Average time per embedding: 0.0052 seconds\n",
      "\n",
      "Performance comparison:\n",
      "Speedup with caching: 16.64x faster\n",
      "Time saved: 0.8196 seconds (94.0%)\n",
      "Latency reduction: 0.0820 seconds per query\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Text to use for benchmarking\n",
    "benchmark_text = \"This is a benchmark text to measure the performance of embedding caching.\"\n",
    "benchmark_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Create a fresh cache for benchmarking\n",
    "benchmark_cache = EmbeddingsCache(\n",
    "    name=\"benchmark_cache\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    ttl=3600  # 1 hour TTL\n",
    ")\n",
    "\n",
    "# Function to get embeddings without caching\n",
    "def get_embedding_without_cache(text, model_name):\n",
    "    return vectorizer.embed(text)\n",
    "\n",
    "# Function to get embeddings with caching\n",
    "def get_embedding_with_cache(text, model_name):\n",
    "    if cached_result := benchmark_cache.get(text=text, model_name=model_name):\n",
    "        return cached_result[\"embedding\"]\n",
    "    \n",
    "    embedding = vectorizer.embed(text)\n",
    "    benchmark_cache.set(\n",
    "        text=text,\n",
    "        model_name=model_name,\n",
    "        embedding=embedding\n",
    "    )\n",
    "    return embedding\n",
    "\n",
    "# Number of iterations for the benchmark\n",
    "n_iterations = 10\n",
    "\n",
    "# Benchmark without caching\n",
    "print(\"Benchmarking without caching:\")\n",
    "start_time = time.time()\n",
    "for _ in tqdm(range(n_iterations)):\n",
    "    _ = get_embedding_without_cache(benchmark_text, benchmark_model)\n",
    "no_cache_time = time.time() - start_time\n",
    "print(f\"Time taken without caching: {no_cache_time:.4f} seconds\")\n",
    "print(f\"Average time per embedding: {no_cache_time/n_iterations:.4f} seconds\")\n",
    "\n",
    "# Benchmark with caching\n",
    "print(\"\\nBenchmarking with caching:\")\n",
    "start_time = time.time()\n",
    "for _ in tqdm(range(n_iterations)):\n",
    "    _ = get_embedding_with_cache(benchmark_text, benchmark_model)\n",
    "cache_time = time.time() - start_time\n",
    "print(f\"Time taken with caching: {cache_time:.4f} seconds\")\n",
    "print(f\"Average time per embedding: {cache_time/n_iterations:.4f} seconds\")\n",
    "\n",
    "# Compare performance\n",
    "speedup = no_cache_time / cache_time\n",
    "latency_reduction = (no_cache_time/n_iterations) - (cache_time/n_iterations)\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"Speedup with caching: {speedup:.2f}x faster\")\n",
    "print(f\"Time saved: {no_cache_time - cache_time:.4f} seconds ({(1 - cache_time/no_cache_time) * 100:.1f}%)\")\n",
    "print(f\"Latency reduction: {latency_reduction:.4f} seconds per query\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Use Cases for Embedding Caching\n",
    "\n",
    "Embedding caching is particularly useful in the following scenarios:\n",
    "\n",
    "1. **Search applications**: Cache embeddings for frequently searched queries to reduce latency\n",
    "2. **Content recommendation systems**: Cache embeddings for content items to speed up similarity calculations\n",
    "3. **API services**: Reduce costs and improve response times when generating embeddings through paid APIs\n",
    "4. **Batch processing**: Speed up processing of datasets that contain duplicate texts\n",
    "5. **Chatbots and virtual assistants**: Cache embeddings for common user queries to provide faster responses\n",
    "6. **Development** workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Let's clean up our caches to avoid leaving data in Redis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up all caches\n",
    "cache.clear()\n",
    "ttl_cache.clear()\n",
    "example_cache.clear()\n",
    "benchmark_cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `EmbeddingsCache` provides an efficient way to store and retrieve embeddings with their associated text and metadata. Key features include:\n",
    "\n",
    "- Simple API for storing and retrieving embeddings\n",
    "- Support for metadata storage alongside embeddings\n",
    "- Configurable time-to-live (TTL) for cache entries\n",
    "- Key-based operations for advanced use cases\n",
    "- Async support for use in asynchronous applications\n",
    "- Significant performance improvements (16x faster in our benchmark)\n",
    "\n",
    "By using the `EmbeddingsCache`, you can reduce computational costs and improve the performance of applications that rely on embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
