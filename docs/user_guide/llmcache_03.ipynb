{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Caching\n",
    "\n",
    "RedisVL provides the ``LLMCache`` interface to turn Redis, with it's vector search capability, into a semantic cache to store query results, thereby reducing the number of requests and tokens sent to the Large Language Models (LLM) service. This decreases expenses and enhances performance by reducing the time taken to generate responses.\n",
    "\n",
    "This notebook will go over how to use ``LLMCache`` for your applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import OpenAI to user their API for responding to prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "if not api_key:\n",
    "  api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "def ask_openai(question):\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",\n",
    "      prompt=question,\n",
    "      max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "print(ask_openai(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and using ``LLMCache``\n",
    "\n",
    "``LLMCache`` will automatically create an index within Redis upon initialization for the semantic cache. The same ``SearchIndex`` class used in the previous tutorials is used here to perform index creation and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.llmcache.semantic import SemanticCache\n",
    "cache = SemanticCache(\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    threshold=0.9, # semantic similarity threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Index Information:\n",
      "╭──────────────┬────────────────┬──────────────┬─────────────────┬────────────╮\n",
      "│ Index Name   │ Storage Type   │ Prefixes     │ Index Options   │   Indexing │\n",
      "├──────────────┼────────────────┼──────────────┼─────────────────┼────────────┤\n",
      "│ cache        │ HASH           │ ['llmcache'] │ []              │          0 │\n",
      "╰──────────────┴────────────────┴──────────────┴─────────────────┴────────────╯\n",
      "Index Fields:\n",
      "╭───────────────┬───────────────┬────────╮\n",
      "│ Name          │ Attribute     │ Type   │\n",
      "├───────────────┼───────────────┼────────┤\n",
      "│ prompt_vector │ prompt_vector │ VECTOR │\n",
      "╰───────────────┴───────────────┴────────╯\n"
     ]
    }
   ],
   "source": [
    "# look at the index specification created for the semantic cache lookup\n",
    "!rvl index info -i cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the cache\n",
    "cache.check(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the question and answer\n",
    "cache.store(\"What is the capital of France?\", \"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the cache again\n",
    "cache.check(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for a semantically similar result\n",
    "cache.check(\"What really is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decrease the semantic similarity threshold\n",
    "cache.set_threshold(0.7)\n",
    "cache.check(\"What really is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adversarial example (not semantically similar enough)\n",
    "cache.check(\"What is the capital of Spain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "Next, we will measure the speedup obtained by using ``LLMCache``. We will use the ``time`` module to measure the time taken to generate responses with and without ``LLMCache``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str):\n",
    "    results = cache.check(question)\n",
    "    if results:\n",
    "        return results[0]\n",
    "    else:\n",
    "        answer = ask_openai(question)\n",
    "        cache.store(question, answer)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken without cache 0.8732700347900391\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "answer = answer_question(\"What is the capital of France?\")\n",
    "end = time.time()\n",
    "print(f\"Time taken without cache {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken with cache: 0.04746699333190918\n",
      "Percentage of time saved: 94.56%\n"
     ]
    }
   ],
   "source": [
    "cached_start = time.time()\n",
    "cached_answer = answer_question(\"What is the capital of France?\")\n",
    "cached_end = time.time()\n",
    "print(f\"Time Taken with cache: {cached_end - cached_start}\")\n",
    "print(f\"Percentage of time saved: {round(((end - start) - (cached_end - cached_start)) / (end - start) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics:\n",
      "╭─────────────────────────────┬─────────────╮\n",
      "│ Stat Key                    │ Value       │\n",
      "├─────────────────────────────┼─────────────┤\n",
      "│ num_docs                    │ 1           │\n",
      "│ num_terms                   │ 0           │\n",
      "│ max_doc_id                  │ 2           │\n",
      "│ num_records                 │ 2           │\n",
      "│ percent_indexed             │ 1           │\n",
      "│ hash_indexing_failures      │ 0           │\n",
      "│ number_of_uses              │ 11          │\n",
      "│ bytes_per_record_avg        │ 0           │\n",
      "│ doc_table_size_mb           │ 0.000134468 │\n",
      "│ inverted_sz_mb              │ 0           │\n",
      "│ key_table_size_mb           │ 2.76566e-05 │\n",
      "│ offset_bits_per_record_avg  │ nan         │\n",
      "│ offset_vectors_sz_mb        │ 0           │\n",
      "│ offsets_per_term_avg        │ 0           │\n",
      "│ records_per_doc_avg         │ 2           │\n",
      "│ sortable_values_size_mb     │ 0           │\n",
      "│ total_indexing_time         │ 0.211       │\n",
      "│ total_inverted_index_blocks │ 11          │\n",
      "│ vector_index_sz_mb          │ 3.00814     │\n",
      "╰─────────────────────────────┴─────────────╯\n"
     ]
    }
   ],
   "source": [
    "# check the stats of the index\n",
    "!rvl stats -i cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the index and all cached items\n",
    "cache.index.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
